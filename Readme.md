# Hello

This is `Ask` Framework.

This is a work in progress public research in a field of Natural Language Driven UI (NLDUI).
Funny fact, that in NLDUI may be spelled like NL DUI, and in NL 'DUI' sounds like 'bye!'.
So, <mark>old frameworks, doei</mark>!

# Ask language

<mark>Ask</mark> is a fundamental language for describing data flows between routines.
<mark>Routines</mark> decomposed to fundamentals, like [read, write, ask, ...etc].
Series of fundamental calls should 40-90% stability cache, so we do not overuse resources.
<mark>Fundamentals</mark> may choose to read or write cache, or create other artifacts.

# For all platforms

<mark>Ask</mark> may be transpiled to any language, because any source code is <mark>text</text>.
You can describe concepts, architectures, ideas, everything you want, in natural language, and then run this routine on any platrofm: from local laptop or p2p connection, to blockchain or 

# Scheduler

File-based scheduler just checks the folder and asks every routine with file content as a payload.
It's simple: I need one payload, I need one temporary storage for it, and I have it - the file itself.
Based on reply, scheduler decides to run routine, skip for now or even delete.

# Low Level

Ask is a low level framework. You can build whatever you want or already have systems for access management, file system management etc. I don't care, it Ask job. You build it on top of Ask, using Ask.

# Default message

For routines empty arguments like `ask()` or `read()` are just the state when they have not enough info to successfully run the routine.
Then they reply with default message, adjusted for the received payload.
:::
For example this may contain a complete ready to run request which will produce asked result.
But client have to ask for it. And now they know how.
:::

## Architecture

On a level of architecture this means that before proceeding to the task, we ask system questions

ask(is it safe to run this routine?)
ask(am I able to run this routine?)
ask(do I want to run this routine?)
ask(do I want to run this routine?)
...etc

This also may include the bid and the offer. So models may just casually argue for the price for a while, until both satisfied.

# Time and money

We don't have fast and cheap AI which at the same time is extremely smart.
But with the same speed of reasoning evolution and price decrease concept described in this file may be ready for real world production usage in a year or less.
Costs should decrease though, while value increases non-linear.

# All you need is readme?

Well, the project now approaching this way.
The only files which I decided only edit myself is this one, 'Readme.md'.
I can read it and produce artifacts and schedule routines using it. 
Or not, up to you.

# Cache

We know for sure that currently, in March 2025, smart AI is slower than any reasonable file storage, from local to cloud or virtual. 
So it makes sense to 

# Too much freedome

Ask elevates and encourages freedome and responsibility.


# Live research

This is a live research, which means, I commit everything I have, no filtering, no hidden work, all public.
I may swear in the comments or even in the code.
Well, swear language is a beautiful example of powerful a Natural Language, and we need to explore swear words usage for model run-time corrections.

This live research may contain errors, wrong concepts, and other temporary results.
I need them for onging work, they are temporary artifacts, and `git` is a beautiful tool for working with such artifacts: texts and text diffs.

# Docs

To build docs, run in terminal
```
npm run routine playground/docs_index.html.ask.js && open playground/docs_index.html
```

# Natural language

Use of Natural Language is encouraged by default.

# File based

`read` and `write` are very common used fundamentals for working with temporary artifacts.
*routines* may produce huge results, media, binary etc, so they usually put a result to a file, and reply with a file name.

I do not implement payloads as arguments, because it encourages to create cache with intermediate results.

# Cache is your app

Having cache basically means having answers for all exact questions.
Then, the needed artifact is built with quantum speed and delivered to client in a moment.
My server controls headers for "old school web" to control cache on the browser side.

# Routing data flows

Routing such operation in sequential or parallel chains, builing new routines on top of existing, unlocks unlimited nested levels and high performance with file based async cache.

# Server

*Ask server* accepts any path and text body for all methods.
It may reply:
- empty string ''
- requested `source routine` cached result
- requested `source routine` in-place (lazy) computed result
- requested `source` text content, if `source` is not a path to routine

# Infrastructure

Server may be deployed to aws lambda.

# Scaling

Decomposing <mark>Intent</mark> into smaller tasks unless it may be executed using <mark>Ask</mark> framework.

# Humans and machines

```
This file is completely made by a human, K. Lekh.
```

So, this makes `Readme.md` so special.
Humans always bring meaning to regular things.

All other files may be <mark>read and modified by AI</mark>.
AI may produce any other file, such as documentation in md, data in json or code in js.
Server may run routines, which are js files, in a safe sandbox.

# Examples
Application is limitless.
I want it to work in a loop and find my unfinished ideas, and finish them for me.
May be even deploy and test them for me. And analyse results. And do iterations.

Does it give freedom?

# Keep in touch

I'm curious about other ai-driven frameworks, so don't shy to send a message:
`hello@k-lekh.com`